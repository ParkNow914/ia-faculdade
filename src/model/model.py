"""
ARQUITETURA DO MODELO DE REGRESS√ÉO ML PARA PREVIS√ÉO DE ENERGIA
Usa algoritmos de Machine Learning tradicionais para previs√£o de consumo.
"""

import numpy as np
import joblib
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    VotingRegressor,
    StackingRegressor
)
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Tentar importar XGBoost (opcional)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    import sys
    if sys.stdout.encoding and 'utf' not in sys.stdout.encoding.lower():
        print("[WARNING] XGBoost nao disponivel. Usando apenas scikit-learn.")
    else:
        print("‚ö†Ô∏è XGBoost n√£o dispon√≠vel. Usando apenas scikit-learn.")


class EnergyRegressionModel:
    """
    Modelo de regress√£o ML para previs√£o de consumo de energia el√©trica.
    Usa ensemble de m√∫ltiplos algoritmos para m√°xima acur√°cia.
    """
    
    def __init__(self):
        """Inicializa o modelo de regress√£o."""
        self.model = None
        self.best_model = None
        self.best_score = None
        self.model_name = None
        
    def create_base_models(self):
        """
        Cria modelos base para ensemble.
        
        Returns:
            Lista de tuplas (nome, modelo)
        """
        models = [
            ('rf', RandomForestRegressor(
                n_estimators=300,  # Aumentado para melhor acur√°cia (ainda r√°pido)
                max_depth=30,      # Profundidade m√°xima para capturar padr√µes complexos
                min_samples_split=2,  # M√≠nimo para m√°xima flexibilidade
                min_samples_leaf=1,   # M√≠nimo para m√°xima flexibilidade
                random_state=42,
                n_jobs=-1,         # Paralelizar m√°ximo para velocidade
                verbose=0,
                max_features='sqrt',
                bootstrap=True,
                oob_score=False,   # Desabilitado para velocidade
                warm_start=False   # Desabilitado para velocidade
            )),
            ('gb', GradientBoostingRegressor(
                n_estimators=300,  # Aumentado para melhor acur√°cia
                max_depth=12,      # Profundidade aumentada
                learning_rate=0.04,  # Learning rate otimizado (balanceado)
                min_samples_split=2,  # M√≠nimo para m√°xima flexibilidade
                min_samples_leaf=1,   # M√≠nimo para m√°xima flexibilidade
                random_state=42,
                verbose=0,
                subsample=0.9,        # Subsampling otimizado
                max_features='sqrt',  # Para melhor performance
                validation_fraction=0.1,  # Valida√ß√£o interna
                n_iter_no_change=20   # Early stopping mais tolerante
            ))
        ]
        
        # Adicionar XGBoost se dispon√≠vel
        if XGBOOST_AVAILABLE:
            models.append(('xgb', xgb.XGBRegressor(
                n_estimators=300,  # Aumentado para melhor acur√°cia
                max_depth=12,      # Profundidade aumentada
                learning_rate=0.04,  # Learning rate otimizado
                random_state=42,
                n_jobs=-1,         # Paralelizar m√°ximo
                verbosity=0,
                subsample=0.9,        # Subsampling otimizado
                colsample_bytree=0.9, # Feature sampling otimizado
                reg_alpha=0.05,       # Regulariza√ß√£o L1 (reduzida)
                reg_lambda=0.5,       # Regulariza√ß√£o L2 (reduzida)
                gamma=0,
                min_child_weight=1,    # M√≠nimo para m√°xima flexibilidade
                early_stopping_rounds=20  # Early stopping
            )))
        
        # Adicionar modelos lineares (otimizados para melhor acur√°cia)
        models.extend([
            ('ridge', Ridge(alpha=0.3)),  # Reduzido para melhor acur√°cia
            ('lasso', Lasso(alpha=0.01, max_iter=2000))  # Reduzido + mais itera√ß√µes
        ])
        
        return models
    
    def create_ensemble_model(self, X_train, y_train):
        """
        Cria e treina um modelo ensemble otimizado usando StackingRegressor
        para m√°xima acur√°cia.
        
        Args:
            X_train: Features de treino
            y_train: Target de treino
            
        Returns:
            Modelo treinado
        """
        print("üèóÔ∏è Criando modelo ensemble de regress√£o (Stacking)...")
        
        # Criar modelos base
        base_models = self.create_base_models()
        
        # Criar meta-learner otimizado (modelo que combina os outros)
        meta_learner = Ridge(alpha=0.3)  # Alpha reduzido para melhor acur√°cia
        
        try:
            ensemble = StackingRegressor(
                estimators=base_models,
                final_estimator=meta_learner,
                cv=3,  # Reduzido de 5 para 3 (mais r√°pido, ainda eficaz)
                n_jobs=-1,  # Paralelizar m√°ximo
                verbose=0,
                passthrough=False  # N√£o passar features originais (mais r√°pido)
            )
            print("‚úÖ Modelo ensemble Stacking criado (m√°xima acur√°cia)!")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao criar StackingRegressor, usando VotingRegressor: {e}")
            # Fallback para VotingRegressor
            n_models = len(base_models)
            if n_models == 4:  # RF, GB, Ridge, Lasso
                weights = [5, 5, 1, 1]  # Pesos otimizados (RF e GB mais importantes)
            elif n_models == 5:  # RF, GB, XGB, Ridge, Lasso
                weights = [5, 5, 5, 1, 1]  # Pesos otimizados
            else:
                weights = None
            
            ensemble = VotingRegressor(
                estimators=base_models,
                weights=weights,
                n_jobs=-1
            )
            print("‚úÖ Modelo ensemble Voting criado!")
        
        return ensemble
    
    def optimize_model(self, X_train, y_train, model_type='ensemble'):
        """
        Otimiza hiperpar√¢metros do modelo usando GridSearch.
        
        Args:
            X_train: Features de treino
            y_train: Target de treino
            model_type: Tipo de modelo ('ensemble', 'rf', 'gb')
            
        Returns:
            Melhor modelo encontrado
        """
        print(f"üîç Otimizando modelo {model_type}...")
        
        if model_type == 'rf':
            model = RandomForestRegressor(random_state=42, n_jobs=-1, verbose=0)
            param_grid = {
                'n_estimators': [250, 300, 350],  # Valores mais altos
                'max_depth': [20, 25, 30],        # Valores mais altos
                'min_samples_split': [2, 3, 4],
                'min_samples_leaf': [1, 2],
                'max_features': ['sqrt', 'log2']  # Adicionado
            }
        elif model_type == 'gb':
            model = GradientBoostingRegressor(random_state=42, verbose=0)
            param_grid = {
                'n_estimators': [250, 300, 350],  # Valores mais altos
                'max_depth': [8, 10, 12],         # Valores mais altos
                'learning_rate': [0.02, 0.03, 0.05],  # Valores mais baixos
                'min_samples_split': [2, 3, 4],
                'subsample': [0.8, 0.9, 1.0]      # Adicionado
            }
        else:
            # Para ensemble, usar modelo padr√£o (GridSearch √© muito lento)
            return self.create_ensemble_model(X_train, y_train)
        
        # GridSearch com valida√ß√£o cruzada
        grid_search = GridSearchCV(
            model,
            param_grid,
            cv=3,
            scoring='neg_mean_absolute_error',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train.ravel())
        
        print(f"‚úÖ Melhor score: {-grid_search.best_score_:.4f}")
        print(f"‚úÖ Melhores par√¢metros: {grid_search.best_params_}")
        
        return grid_search.best_estimator_
    
    def train(self, X_train, y_train, X_val, y_val, optimize=False, model_type='ensemble'):
        """
        Treina o modelo de regress√£o.
        
        Args:
            X_train: Features de treino
            y_train: Target de treino
            X_val: Features de valida√ß√£o
            y_val: Target de valida√ß√£o
            optimize: Se True, otimiza hiperpar√¢metros
            model_type: Tipo de modelo ('ensemble', 'rf', 'gb')
        """
        print(f"\nüöÄ Iniciando treinamento do modelo {model_type}...")
        print(f"‚öôÔ∏è Dados de treino: {X_train.shape}")
        print(f"‚öôÔ∏è Dados de valida√ß√£o: {X_val.shape}")
        
        # Ajustar formato do target se necess√°rio
        if len(y_train.shape) > 1:
            y_train = y_train.ravel()
        if len(y_val.shape) > 1:
            y_val = y_val.ravel()
        
        # Criar ou otimizar modelo
        if optimize:
            self.model = self.optimize_model(X_train, y_train, model_type)
        else:
            if model_type == 'ensemble':
                self.model = self.create_ensemble_model(X_train, y_train)
            elif model_type == 'rf':
                self.model = RandomForestRegressor(
                    n_estimators=300,  # Aumentado para melhor acur√°cia
                    max_depth=30,      # Profundidade m√°xima
                    min_samples_split=2,  # M√≠nimo para m√°xima flexibilidade
                    min_samples_leaf=1,   # M√≠nimo para m√°xima flexibilidade
                    random_state=42,
                    n_jobs=-1,         # Paralelizar m√°ximo
                    verbose=0,
                    max_features='sqrt',
                    bootstrap=True,
                    oob_score=False    # Desabilitado para velocidade
                )
            elif model_type == 'gb':
                self.model = GradientBoostingRegressor(
                    n_estimators=300,  # Aumentado para melhor acur√°cia
                    max_depth=12,      # Profundidade aumentada
                    learning_rate=0.04,  # Learning rate otimizado
                    min_samples_split=2,  # M√≠nimo para m√°xima flexibilidade
                    min_samples_leaf=1,   # M√≠nimo para m√°xima flexibilidade
                    random_state=42,
                    verbose=0,
                    subsample=0.9,        # Subsampling otimizado
                    max_features='sqrt',   # Feature sampling
                    validation_fraction=0.1,
                    n_iter_no_change=20
                )
        
        # Treinar modelo
        print("üéØ Treinando modelo...")
        self.model.fit(X_train, y_train)
        
        # Avaliar no conjunto de valida√ß√£o
        y_pred_val = self.model.predict(X_val)
        mae = mean_absolute_error(y_val, y_pred_val)
        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
        r2 = r2_score(y_val, y_pred_val)
        
        print("\n‚úÖ Treinamento conclu√≠do!")
        print(f"üìä M√©tricas de valida√ß√£o:")
        print(f"  MAE: {mae:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  R¬≤: {r2:.4f}")
        
        self.model_name = model_type
        self.best_score = mae
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2
        }
    
    def evaluate(self, X_test, y_test):
        """
        Avalia o modelo no conjunto de teste.
        
        Args:
            X_test: Features de teste
            y_test: Target de teste
            
        Returns:
            Dicion√°rio com m√©tricas
        """
        print("\nüìä Avaliando modelo no conjunto de teste...")
        
        if len(y_test.shape) > 1:
            y_test = y_test.ravel()
        
        y_pred = self.model.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100
        
        print(f"‚úÖ MAE: {mae:.4f}")
        print(f"‚úÖ RMSE: {rmse:.4f}")
        print(f"‚úÖ R¬≤: {r2:.4f}")
        print(f"‚úÖ MAPE: {mape:.2f}%")
        
        return {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'mape': mape
        }
    
    def predict(self, X):
        """
        Faz previs√µes.
        
        Args:
            X: Features (n_samples, n_features)
            
        Returns:
            Previs√µes (n_samples,)
        """
        return self.model.predict(X)
    
    def save_model(self, filepath='src/model/saved_models/regression_model.pkl'):
        """
        Salva o modelo treinado.
        
        Args:
            filepath: Caminho para salvar o modelo
        """
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        joblib.dump(self.model, filepath)
        print(f"üíæ Modelo salvo em: {filepath}")
    
    def load_model(self, filepath='src/model/saved_models/regression_model.pkl'):
        """
        Carrega um modelo salvo.
        
        Args:
            filepath: Caminho do modelo salvo
        """
        self.model = joblib.load(filepath)
        print(f"üìÇ Modelo carregado de: {filepath}")
    
    def get_model_info(self):
        """
        Retorna informa√ß√µes sobre o modelo.
        
        Returns:
            Dicion√°rio com informa√ß√µes do modelo
        """
        if self.model is None:
            return {
                'status': 'not_trained',
                'message': 'Modelo n√£o foi treinado ainda.'
            }
        
        info = {
            'model_type': self.model_name or 'ensemble',
            'status': 'trained'
        }
        
        # Informa√ß√µes espec√≠ficas por tipo de modelo
        if hasattr(self.model, 'n_estimators'):
            info['n_estimators'] = self.model.n_estimators
        if hasattr(self.model, 'estimators_'):
            info['n_base_models'] = len(self.model.estimators_)
        if hasattr(self.model, 'feature_importances_'):
            info['n_features'] = len(self.model.feature_importances_)
        
        return info


def create_default_model():
    """
    Cria um modelo de regress√£o com configura√ß√µes padr√£o otimizadas.
    
    Returns:
        Inst√¢ncia do modelo
    """
    model = EnergyRegressionModel()
    return model


# Importar os para criar diret√≥rio
import os

if __name__ == "__main__":
    # Teste da arquitetura
    print("üß™ Testando arquitetura do modelo de regress√£o...")
    
    model = create_default_model()
    
    # Dados dummy para teste
    X_dummy = np.random.rand(100, 30)
    y_dummy = np.random.rand(100)
    
    print("\nüîç Testando predi√ß√£o...")
    # Criar modelo simples para teste
    test_model = RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1)
    test_model.fit(X_dummy, y_dummy)
    predictions = test_model.predict(X_dummy[:5])
    print(f"‚úÖ Shape das predi√ß√µes: {predictions.shape}")
    
    print("\n‚úÖ Teste conclu√≠do!")
